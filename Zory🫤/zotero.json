[
  {"id":"agarwalReinforcementLearningTheory","author":[{"family":"Agarwal","given":"Alekh"},{"family":"Jiang","given":"Nan"},{"family":"Kakade","given":"Sham M"},{"family":"Sun","given":"Wen"}],"citation-key":"agarwalReinforcementLearningTheory","language":"en","source":"Zotero","title":"Reinforcement Learning: Theory and Algorithms","type":"book"},
  {"id":"blumFoundationsDataScience","author":[{"family":"Blum","given":"Avrim"},{"family":"Hopcroft","given":"John"},{"family":"Kannan","given":"Ravindran"}],"citation-key":"blumFoundationsDataScience","language":"en","source":"Zotero","title":"Foundations of Data Science","type":"article-journal"},
  {"id":"borgwardtGraphKernelsStateoftheArt2020","abstract":"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Borgwardt","given":"Karsten"},{"family":"Ghisu","given":"Elisabetta"},{"family":"Llinares-López","given":"Felipe"},{"family":"O'Bray","given":"Leslie"},{"family":"Rieck","given":"Bastian"}],"citation-key":"borgwardtGraphKernelsStateoftheArt2020","container-title":"Foundations and Trends® in Machine Learning","container-title-short":"FNT in Machine Learning","DOI":"10.1561/2200000076","ISSN":"1935-8237, 1935-8245","issue":"5-6","issued":{"date-parts":[[2020]]},"page":"531-712","source":"arXiv.org","title":"Graph Kernels: State-of-the-Art and Future Challenges","title-short":"Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/2011.03854","volume":"13"},
  {"id":"chamberlainGraphNeuralNetworks2022","abstract":"Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Chamberlain","given":"Benjamin Paul"},{"family":"Shirobokov","given":"Sergey"},{"family":"Rossi","given":"Emanuele"},{"family":"Frasca","given":"Fabrizio"},{"family":"Markovich","given":"Thomas"},{"family":"Hammerla","given":"Nils"},{"family":"Bronstein","given":"Michael M."},{"family":"Hansmire","given":"Max"}],"citation-key":"chamberlainGraphNeuralNetworks2022","issued":{"date-parts":[[2022,10,3]]},"number":"arXiv:2209.15486","publisher":"arXiv","source":"arXiv.org","title":"Graph Neural Networks for Link Prediction with Subgraph Sketching","type":"article","URL":"http://arxiv.org/abs/2209.15486"},
  {"id":"ghojoghReproducingKernelHilbert2021","abstract":"This is a tutorial and survey paper on kernels, kernel methods, and related fields. We start with reviewing the history of kernels in functional analysis and machine learning. Then, Mercer kernel, Hilbert and Banach spaces, Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof, frequently used kernels, kernel construction from distance metric, important classes of kernels (including bounded, integrally positive definite, universal, stationary, and characteristic kernels), kernel centering and normalization, and eigenfunctions are explained in detail. Then, we introduce types of use of kernels in machine learning including kernel methods (such as kernel support vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt independence criterion, maximum mean discrepancy, kernel mean embedding, and kernel dimensionality reduction. We also cover rank and factorization of kernel matrix as well as the approximation of eigenfunctions and kernels using the Nystr{\\\"o}m method. This paper can be useful for various fields of science including machine learning, dimensionality reduction, functional analysis in mathematics, and mathematical physics in quantum mechanics.","accessed":{"date-parts":[[2023,1,10]]},"author":[{"family":"Ghojogh","given":"Benyamin"},{"family":"Ghodsi","given":"Ali"},{"family":"Karray","given":"Fakhri"},{"family":"Crowley","given":"Mark"}],"citation-key":"ghojoghReproducingKernelHilbert2021","issued":{"date-parts":[[2021,6,15]]},"number":"arXiv:2106.08443","publisher":"arXiv","source":"arXiv.org","title":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey","title-short":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning","type":"article","URL":"http://arxiv.org/abs/2106.08443"},
  {"id":"huStrategiesPretrainingGraph2020","abstract":"Many domains in machine learning have datasets with a large number of related but different tasks. Those domains are challenging because task-specific labels are often scarce and test examples can be distributionally different from examples seen during training. An effective solution to these challenges is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective for improving many language and vision domains, pre-training on graph datasets remains an open question. Here, we develop a strategy for pre-training Graph Neural Networks (GNNs). Crucial to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs. We systematically study different pre-training strategies on multiple datasets and find that when ad-hoc strategies are applied, pre-trained GNNs often exhibit negative transfer and perform worse than non-pre-trained GNNs on many downstream tasks. In contrast, our proposed strategy is effective and avoids negative transfer across downstream tasks, leading up to 11.7% absolute improvements in ROC-AUC over non-pre-trained models and achieving state of the art performance.","author":[{"family":"Hu","given":"Weihua"},{"family":"Liu","given":"Bowen"},{"family":"Gomes","given":"Joseph"},{"literal":"Joseph Gomes"},{"family":"Zitnik","given":"Marinka"},{"family":"Liang","given":"Percy"},{"family":"Pande","given":"Vijay S."},{"family":"Leskovec","given":"Jure"}],"citation-key":"huStrategiesPretrainingGraph2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 1905.12265\nMAG ID: 2996604169\nS2ID: 789a7069d1a2d02d784e4821685b216cc63e6ec8","title":"Strategies for Pre-training Graph Neural Networks","type":"article-journal"},
  {"id":"kriegeSurveyGraphKernels2020","abstract":"Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Kriege","given":"Nils M."},{"family":"Johansson","given":"Fredrik D."},{"family":"Morris","given":"Christopher"}],"citation-key":"kriegeSurveyGraphKernels2020","container-title":"Applied Network Science","container-title-short":"Appl Netw Sci","DOI":"10.1007/s41109-019-0195-3","ISSN":"2364-8228","issue":"1","issued":{"date-parts":[[2020,12]]},"page":"6","source":"arXiv.org","title":"A Survey on Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/1903.11835","volume":"5"},
  {"id":"libkinElementsFiniteModel2004","accessed":{"date-parts":[[2023,3,10]]},"author":[{"family":"Libkin","given":"Leonid"}],"citation-key":"libkinElementsFiniteModel2004","DOI":"10.1007/978-3-662-07003-1","event-place":"Berlin, Heidelberg","ISBN":"978-3-642-05948-3 978-3-662-07003-1","issued":{"date-parts":[[2004]]},"language":"en","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Elements of Finite Model Theory","type":"book","URL":"http://link.springer.com/10.1007/978-3-662-07003-1"},
  {"id":"liuGraphConvolutionalNetworks2021","abstract":"Graph neural networks (GNNs) and message passing neural networks (MPNNs) have been proven to be expressive for subgraph structures in many applications. Some applications in heterogeneous graphs require explicit edge modeling, such as subgraph isomorphism counting and matching. However, existing message passing mechanisms are not designed well in theory. In this paper, we start from a particular edge-to-vertex transform and exploit the isomorphism property in the edge-to-vertex dual graphs. We prove that searching isomorphisms on the original graph is equivalent to searching on its dual graph. Based on this observation, we propose dual message passing neural networks (DMPNNs) to enhance the substructure representation learning in an asynchronous way for subgraph isomorphism counting and matching as well as unsupervised node classification. Extensive experiments demonstrate the robust performance of DMPNNs by combining both node and edge representation learning in synthetic and real heterogeneous graphs. Code is available at https://github.com/HKUST-KnowComp/DualMessagePassing.","accessed":{"date-parts":[[2023,2,19]]},"author":[{"family":"Liu","given":"Xin"},{"family":"Song","given":"Yangqiu"}],"citation-key":"liuGraphConvolutionalNetworks2021","DOI":"10.48550/arXiv.2112.08764","issued":{"date-parts":[[2021,12,16]]},"number":"arXiv:2112.08764","publisher":"arXiv","source":"arXiv.org","title":"Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching","type":"article","URL":"http://arxiv.org/abs/2112.08764"},
  {"id":"mohriFoundationsMachineLearning","author":[{"family":"Mohri","given":"Mehryar"},{"family":"Rostamizadeh","given":"Afshin"},{"family":"Talwalkar","given":"Ameet"}],"citation-key":"mohriFoundationsMachineLearning","language":"en","source":"Zotero","title":"Foundations of Machine Learning (second edition)","type":"article-journal"},
  {"id":"morrisLearningGraphsKernel2019","abstract":"Linked data arise in many real-world settings - from chemical molecules, and protein-protein interaction networks, to social networks. Hence, in recent years, machine learning methods for graphs have become an important area of research. The present work deals with (supervised) graph classification, i.e., given a set of (class) labeled graphs we want to train a model such that it can predict the labels of unlabeled graphs. Thereto, we want to map a graph to a (usually) high-dimensional vector such that standard machine learning techniques can be applied.  In the first part of this thesis, we present kernel methods for graphs. Specifically, we present a framework for scalable kernels that can handle continuous vertex and edge labels, which often arise with real-world graphs, e.g., chemical and physical properties of atoms. Moreover, we present a graph kernel which can take global or higher-order graph properties into account, usually not captured by other graph kernels. To this end, we propose a local version of the $k$-dimensional Weisfeiler-Leman algorithm, which is a well-known heuristic for the graph isomorphism problem. We show that a variant of our local algorithm has at least the same power as the original algorithm, while at the same time taking the sparsity of the underlying graph into account and preventing overfitting. To make this kernel scalable, we present an approximation algorithm with theoretical guarantees. Subsequently, we introduce a theoretical framework for the analysis of graph kernels showing that most kernels are not capable of distinguishing simple graph-theoretic properties and propose a theoretically well-founded graph kernel, which can distinguish these properties. The second part of this thesis deals with neural approaches to graph classification and their connection to kernel methods. We show that the expressivity of so-called graph neural networks can be upper-bounded by the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic. We then leverage this insight to propose a generalization which is provable more powerful than graph neural networks regarding distinguishing non-isomorphic graphs.","accessed":{"date-parts":[[2023,3,10]]},"author":[{"family":"Morris","given":"Christopher"}],"citation-key":"morrisLearningGraphsKernel2019","DOI":"10.17877/DE290R-20637","issued":{"date-parts":[[2019]]},"language":"en","note":"Accepted: 2020-02-19T06:22:38Z","source":"eldorado.tu-dortmund.de","title":"Learning with graphs: kernel and neural approaches","title-short":"Learning with graphs","type":"article-journal","URL":"https://eldorado.tu-dortmund.de/handle/2003/38718"},
  {"id":"ottoGradedModalLogic2019","abstract":"This note sketches the extension of the basic characterisation theorems as the bisimulation-invariant fragment of first-order logic to modal logic with graded modalities and matching adaptation of bisimulation. We focus on showing expressive completeness of graded multi-modal logic for those first-order properties of pointed Kripke structures that are preserved under counting bisimulation equivalence among all or among just all finite pointed Kripke structures.","accessed":{"date-parts":[[2023,3,23]]},"author":[{"family":"Otto","given":"Martin"}],"citation-key":"ottoGradedModalLogic2019","DOI":"10.48550/arXiv.1910.00039","issued":{"date-parts":[[2019,9,30]]},"number":"arXiv:1910.00039","publisher":"arXiv","source":"arXiv.org","title":"Graded modal logic and counting bisimulation","type":"article","URL":"http://arxiv.org/abs/1910.00039"},
  {"id":"perezFiLMVisualReasoning2017","abstract":"We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","accessed":{"date-parts":[[2023,2,22]]},"author":[{"family":"Perez","given":"Ethan"},{"family":"Strub","given":"Florian"},{"family":"Vries","given":"Harm","non-dropping-particle":"de"},{"family":"Dumoulin","given":"Vincent"},{"family":"Courville","given":"Aaron"}],"citation-key":"perezFiLMVisualReasoning2017","issued":{"date-parts":[[2017,12,18]]},"number":"arXiv:1709.07871","publisher":"arXiv","source":"arXiv.org","title":"FiLM: Visual Reasoning with a General Conditioning Layer","title-short":"FiLM","type":"article","URL":"http://arxiv.org/abs/1709.07871"},
  {"id":"qiuLogicalExpressivenessGraph2023","abstract":"Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the development of an entity labeling method for capturing difficult logical rules. Experimental results are consistent with our theoretical results and verify the effectiveness of our proposed method.","accessed":{"date-parts":[[2023,3,23]]},"author":[{"family":"Qiu","given":"Haiquan"},{"family":"Zhang","given":"Yongqi"},{"family":"Li","given":"Yong"},{"family":"Yao","given":"Quanming"}],"citation-key":"qiuLogicalExpressivenessGraph2023","DOI":"10.48550/arXiv.2303.12306","issued":{"date-parts":[[2023,3,22]]},"number":"arXiv:2303.12306","publisher":"arXiv","source":"arXiv.org","title":"Logical Expressiveness of Graph Neural Network for Knowledge Graph Reasoning","type":"article","URL":"http://arxiv.org/abs/2303.12306"},
  {"id":"renQuery2boxReasoningKnowledge2020","abstract":"Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on two large KGs and show that query2box achieves up to 25% relative improvement over the state of the art.","author":[{"family":"Ren","given":"Hongyu"},{"family":"Hu","given":"Weihua"},{"family":"Leskovec","given":"Jure"}],"citation-key":"renQuery2boxReasoningKnowledge2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 2002.05969\nMAG ID: 2994695355\nS2ID: 66f7823be7a0adcd8ed3ecebf00e53daf45bc6bc","title":"Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings","type":"article-journal"},
  {"id":"schwabeCardinalityEstimationKnowledge2023","abstract":"Cardinality Estimation over Knowledge Graphs (KG) is crucial for query optimization, yet remains a challenging task due to the semi-structured nature and complex correlations of typical Knowledge Graphs. In this work, we propose GNCE, a novel approach that leverages knowledge graph embeddings and Graph Neural Networks (GNN) to accurately predict the cardinality of conjunctive queries. GNCE first creates semantically meaningful embeddings for all entities in the KG, which are then integrated into the given query, which is processed by a GNN to estimate the cardinality of the query. We evaluate GNCE on several KGs in terms of q-Error and demonstrate that it outperforms state-of-the-art approaches based on sampling, summaries, and (machine) learning in terms of estimation accuracy while also having lower execution time and less parameters. Additionally, we show that GNCE can inductively generalise to unseen entities, making it suitable for use in dynamic query processing scenarios. Our proposed approach has the potential to significantly improve query optimization and related applications that rely on accurate cardinality estimates of conjunctive queries.","accessed":{"date-parts":[[2023,3,5]]},"author":[{"family":"Schwabe","given":"Tim"},{"family":"Acosta","given":"Maribel"}],"citation-key":"schwabeCardinalityEstimationKnowledge2023","issued":{"date-parts":[[2023,3,2]]},"number":"arXiv:2303.01140","publisher":"arXiv","source":"arXiv.org","title":"Cardinality Estimation over Knowledge Graphs with Embeddings and Graph Neural Networks","type":"article","URL":"http://arxiv.org/abs/2303.01140"},
  {"id":"wangLogicalMessagePassing2023","abstract":"Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers' embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph represenation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields the new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Wang","given":"Zihao"},{"family":"Song","given":"Yangqiu"},{"family":"Wong","given":"Ginny Y."},{"family":"See","given":"Simon"}],"citation-key":"wangLogicalMessagePassing2023","DOI":"10.48550/arXiv.2301.08859","issued":{"date-parts":[[2023,2,20]]},"number":"arXiv:2301.08859","publisher":"arXiv","source":"arXiv.org","title":"Logical Message Passing Networks with One-hop Inference on Atomic Formulas","type":"article","URL":"http://arxiv.org/abs/2301.08859"},
  {"id":"yangRethinkingKnowledgeGraph2022","abstract":"Most knowledge graphs (KGs) are incomplete, which motivates one important research topic on automatically complementing knowledge graphs. However, evaluation of knowledge graph completion (KGC) models often ignores the incompleteness -- facts in the test set are ranked against all unknown triplets which may contain a large number of missing facts not included in the KG yet. Treating all unknown triplets as false is called the closed-world assumption. This closed-world assumption might negatively affect the fairness and consistency of the evaluation metrics. In this paper, we study KGC evaluation under a more realistic setting, namely the open-world assumption, where unknown triplets are considered to include many missing facts not included in the training or test sets. For the currently most used metrics such as mean reciprocal rank (MRR) and Hits@K, we point out that their behavior may be unexpected under the open-world assumption. Specifically, with not many missing facts, their numbers show a logarithmic trend with respect to the true strength of the model, and thus, the metric increase could be insignificant in terms of reflecting the true model improvement. Further, considering the variance, we show that the degradation in the reported numbers may result in incorrect comparisons between different models, where stronger models may have lower metric numbers. We validate the phenomenon both theoretically and experimentally. Finally, we suggest possible causes and solutions for this problem. Our code and data are available at https://github.com/GraphPKU/Open-World-KG .","accessed":{"date-parts":[[2023,3,6]]},"author":[{"family":"Yang","given":"Haotong"},{"family":"Lin","given":"Zhouchen"},{"family":"Zhang","given":"Muhan"}],"citation-key":"yangRethinkingKnowledgeGraph2022","DOI":"10.48550/arXiv.2209.08858","issued":{"date-parts":[[2022,9,19]]},"number":"arXiv:2209.08858","publisher":"arXiv","source":"arXiv.org","title":"Rethinking Knowledge Graph Evaluation Under the Open-World Assumption","type":"article","URL":"http://arxiv.org/abs/2209.08858"},
  {"id":"yingsuMICOMultialternativeContrastive2022","abstract":"Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a M ulti-alternative contrast I ve learning framework on CO mmonsense knowledge graphs (MICO). MICO generates the commonsense knowledge representation by contextual interaction between entity nodes and relations with multi-alternative contrastive learning. In MICO, the head and tail entities in an ( h, r, t ) knowledge triple are converted to two relation-aware sequence pairs (a premise and an alternative) in the form of natural language. Semantic representations generated by MICO can beneﬁt the following two tasks by simply comparing the distance score between the representations: Extensive experiments show the effectiveness our method.","author":[{"literal":"Ying Su"},{"literal":"Zihao Wang"},{"literal":"Tianqing Fang"},{"literal":"Hongming Zhang"},{"literal":"Yangqiu Song"},{"literal":"T. Zhang"}],"citation-key":"yingsuMICOMultialternativeContrastive2022","container-title":"ArXiv","DOI":"10.48550/arxiv.2210.07570","issued":{"date-parts":[[2022]]},"note":"ARXIV_ID: 2210.07570\nS2ID: 480e667c50eb71ba78bfde47e4686ca7b21148bd","title":"MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation","type":"article-journal"},
  {"id":"yuLearningCountIsomorphisms2023","abstract":"Subgraph isomorphism counting is an important problem on graphs, as many graph-based tasks exploit recurring subgraph patterns. Classical methods usually boil down to a backtracking framework that needs to navigate a huge search space with prohibitive computational costs. Some recent studies resort to graph neural networks (GNNs) to learn a low-dimensional representation for both the query and input graphs, in order to predict the number of subgraph isomorphisms on the input graph. However, typical GNNs employ a node-centric message passing scheme that receives and aggregates messages on nodes, which is inadequate in complex structure matching for isomorphism counting. Moreover, on an input graph, the space of possible query graphs is enormous, and different parts of the input graph will be triggered to match different queries. Thus, expecting a fixed representation of the input graph to match diversely structured query graphs is unrealistic. In this paper, we propose a novel GNN called Count-GNN for subgraph isomorphism counting, to deal with the above challenges. At the edge level, given that an edge is an atomic unit of encoding graph structures, we propose an edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency to preserve fine-grained structural information. At the graph level, we modulate the input graph representation conditioned on the query, so that the input graph can be adapted to each query individually to improve their matching. Finally, we conduct extensive experiments on a number of benchmark datasets to demonstrate the superior performance of Count-GNN.","accessed":{"date-parts":[[2023,2,12]]},"author":[{"family":"Yu","given":"Xingtong"},{"family":"Liu","given":"Zemin"},{"family":"Fang","given":"Yuan"},{"family":"Zhang","given":"Xinming"}],"citation-key":"yuLearningCountIsomorphisms2023","issued":{"date-parts":[[2023,2,7]]},"number":"arXiv:2302.03266","publisher":"arXiv","source":"arXiv.org","title":"Learning to Count Isomorphisms with Graph Neural Networks","type":"article","URL":"http://arxiv.org/abs/2302.03266"},
  {"id":"zhangAddingConditionalControl2023","abstract":"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Zhang","given":"Lvmin"},{"family":"Agrawala","given":"Maneesh"}],"citation-key":"zhangAddingConditionalControl2023","issued":{"date-parts":[[2023,2,10]]},"number":"arXiv:2302.05543","publisher":"arXiv","source":"arXiv.org","title":"Adding Conditional Control to Text-to-Image Diffusion Models","type":"article","URL":"http://arxiv.org/abs/2302.05543"},
  {"id":"zhangCompleteExpressivenessHierarchy2023","abstract":"Recently, subgraph GNNs have emerged as an important direction for developing expressive graph neural networks (GNNs). While numerous architectures have been proposed, so far there is still a limited understanding of how various design paradigms differ in terms of expressive power, nor is it clear what design principle achieves maximal expressiveness with minimal architectural complexity. Targeting these fundamental questions, this paper conducts a systematic study of general node-based subgraph GNNs through the lens of Subgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build a complete hierarchy of SWL with strictly growing expressivity. Concretely, we prove that any node-based subgraph GNN falls into one of the six SWL equivalence classes, among which $\\mathsf{SSWL}$ achieves the maximal expressive power. We also study how these equivalence classes differ in terms of their practical expressiveness such as encoding graph distance and biconnectivity. In addition, we give a tight expressivity upper bound of all SWL algorithms by establishing a close relation with localized versions of Folklore WL tests (FWL). Overall, our results provide insights into the power of existing subgraph GNNs, guide the design of new architectures, and point out their limitations by revealing an inherent gap with the 2-FWL test. Finally, experiments on the ZINC benchmark demonstrate that $\\mathsf{SSWL}$-inspired subgraph GNNs can significantly outperform prior architectures despite great simplicity.","accessed":{"date-parts":[[2023,3,19]]},"author":[{"family":"Zhang","given":"Bohang"},{"family":"Feng","given":"Guhao"},{"family":"Du","given":"Yiheng"},{"family":"He","given":"Di"},{"family":"Wang","given":"Liwei"}],"citation-key":"zhangCompleteExpressivenessHierarchy2023","DOI":"10.48550/arXiv.2302.07090","issued":{"date-parts":[[2023,2,14]]},"number":"arXiv:2302.07090","publisher":"arXiv","source":"arXiv.org","title":"A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests","type":"article","URL":"http://arxiv.org/abs/2302.07090"},
  {"id":"zhangRethinkingExpressivePower2023a","abstract":"Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.","accessed":{"date-parts":[[2023,2,19]]},"author":[{"family":"Zhang","given":"Bohang"},{"family":"Luo","given":"Shengjie"},{"family":"Wang","given":"Liwei"},{"family":"He","given":"Di"}],"citation-key":"zhangRethinkingExpressivePower2023a","DOI":"10.48550/arXiv.2301.09505","issued":{"date-parts":[[2023,1,23]]},"number":"arXiv:2301.09505","publisher":"arXiv","source":"arXiv.org","title":"Rethinking the Expressive Power of GNNs via Graph Biconnectivity","type":"article","URL":"http://arxiv.org/abs/2301.09505"}
]
