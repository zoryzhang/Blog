[
  {"id":"blumFoundationsDataScience","author":[{"family":"Blum","given":"Avrim"},{"family":"Hopcroft","given":"John"},{"family":"Kannan","given":"Ravindran"}],"citation-key":"blumFoundationsDataScience","language":"en","source":"Zotero","title":"Foundations of Data Science","type":"article-journal"},
  {"id":"borgwardtGraphKernelsStateoftheArt2020","abstract":"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Borgwardt","given":"Karsten"},{"family":"Ghisu","given":"Elisabetta"},{"family":"Llinares-López","given":"Felipe"},{"family":"O'Bray","given":"Leslie"},{"family":"Rieck","given":"Bastian"}],"citation-key":"borgwardtGraphKernelsStateoftheArt2020","container-title":"Foundations and Trends® in Machine Learning","container-title-short":"FNT in Machine Learning","DOI":"10.1561/2200000076","ISSN":"1935-8237, 1935-8245","issue":"5-6","issued":{"date-parts":[[2020]]},"page":"531-712","source":"arXiv.org","title":"Graph Kernels: State-of-the-Art and Future Challenges","title-short":"Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/2011.03854","volume":"13"},
  {"id":"chamberlainGraphNeuralNetworks2022","abstract":"Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Chamberlain","given":"Benjamin Paul"},{"family":"Shirobokov","given":"Sergey"},{"family":"Rossi","given":"Emanuele"},{"family":"Frasca","given":"Fabrizio"},{"family":"Markovich","given":"Thomas"},{"family":"Hammerla","given":"Nils"},{"family":"Bronstein","given":"Michael M."},{"family":"Hansmire","given":"Max"}],"citation-key":"chamberlainGraphNeuralNetworks2022","issued":{"date-parts":[[2022,10,3]]},"number":"arXiv:2209.15486","publisher":"arXiv","source":"arXiv.org","title":"Graph Neural Networks for Link Prediction with Subgraph Sketching","type":"article","URL":"http://arxiv.org/abs/2209.15486"},
  {"id":"ghojoghReproducingKernelHilbert2021","abstract":"This is a tutorial and survey paper on kernels, kernel methods, and related fields. We start with reviewing the history of kernels in functional analysis and machine learning. Then, Mercer kernel, Hilbert and Banach spaces, Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof, frequently used kernels, kernel construction from distance metric, important classes of kernels (including bounded, integrally positive definite, universal, stationary, and characteristic kernels), kernel centering and normalization, and eigenfunctions are explained in detail. Then, we introduce types of use of kernels in machine learning including kernel methods (such as kernel support vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt independence criterion, maximum mean discrepancy, kernel mean embedding, and kernel dimensionality reduction. We also cover rank and factorization of kernel matrix as well as the approximation of eigenfunctions and kernels using the Nystr{\\\"o}m method. This paper can be useful for various fields of science including machine learning, dimensionality reduction, functional analysis in mathematics, and mathematical physics in quantum mechanics.","accessed":{"date-parts":[[2023,1,10]]},"author":[{"family":"Ghojogh","given":"Benyamin"},{"family":"Ghodsi","given":"Ali"},{"family":"Karray","given":"Fakhri"},{"family":"Crowley","given":"Mark"}],"citation-key":"ghojoghReproducingKernelHilbert2021","issued":{"date-parts":[[2021,6,15]]},"number":"arXiv:2106.08443","publisher":"arXiv","source":"arXiv.org","title":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey","title-short":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning","type":"article","URL":"http://arxiv.org/abs/2106.08443"},
  {"id":"huStrategiesPretrainingGraph2020","abstract":"Many domains in machine learning have datasets with a large number of related but different tasks. Those domains are challenging because task-specific labels are often scarce and test examples can be distributionally different from examples seen during training. An effective solution to these challenges is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective for improving many language and vision domains, pre-training on graph datasets remains an open question. Here, we develop a strategy for pre-training Graph Neural Networks (GNNs). Crucial to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs. We systematically study different pre-training strategies on multiple datasets and find that when ad-hoc strategies are applied, pre-trained GNNs often exhibit negative transfer and perform worse than non-pre-trained GNNs on many downstream tasks. In contrast, our proposed strategy is effective and avoids negative transfer across downstream tasks, leading up to 11.7% absolute improvements in ROC-AUC over non-pre-trained models and achieving state of the art performance.","author":[{"family":"Hu","given":"Weihua"},{"family":"Liu","given":"Bowen"},{"family":"Gomes","given":"Joseph"},{"literal":"Joseph Gomes"},{"family":"Zitnik","given":"Marinka"},{"family":"Liang","given":"Percy"},{"family":"Pande","given":"Vijay S."},{"family":"Leskovec","given":"Jure"}],"citation-key":"huStrategiesPretrainingGraph2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 1905.12265\nMAG ID: 2996604169\nS2ID: 789a7069d1a2d02d784e4821685b216cc63e6ec8","title":"Strategies for Pre-training Graph Neural Networks","type":"article-journal"},
  {"id":"kriegeSurveyGraphKernels2020","abstract":"Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Kriege","given":"Nils M."},{"family":"Johansson","given":"Fredrik D."},{"family":"Morris","given":"Christopher"}],"citation-key":"kriegeSurveyGraphKernels2020","container-title":"Applied Network Science","container-title-short":"Appl Netw Sci","DOI":"10.1007/s41109-019-0195-3","ISSN":"2364-8228","issue":"1","issued":{"date-parts":[[2020,12]]},"page":"6","source":"arXiv.org","title":"A Survey on Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/1903.11835","volume":"5"},
  {"id":"liuGraphConvolutionalNetworks2021","abstract":"Graph neural networks (GNNs) and message passing neural networks (MPNNs) have been proven to be expressive for subgraph structures in many applications. Some applications in heterogeneous graphs require explicit edge modeling, such as subgraph isomorphism counting and matching. However, existing message passing mechanisms are not designed well in theory. In this paper, we start from a particular edge-to-vertex transform and exploit the isomorphism property in the edge-to-vertex dual graphs. We prove that searching isomorphisms on the original graph is equivalent to searching on its dual graph. Based on this observation, we propose dual message passing neural networks (DMPNNs) to enhance the substructure representation learning in an asynchronous way for subgraph isomorphism counting and matching as well as unsupervised node classification. Extensive experiments demonstrate the robust performance of DMPNNs by combining both node and edge representation learning in synthetic and real heterogeneous graphs. Code is available at https://github.com/HKUST-KnowComp/DualMessagePassing.","accessed":{"date-parts":[[2023,2,19]]},"author":[{"family":"Liu","given":"Xin"},{"family":"Song","given":"Yangqiu"}],"citation-key":"liuGraphConvolutionalNetworks2021","DOI":"10.48550/arXiv.2112.08764","issued":{"date-parts":[[2021,12,16]]},"number":"arXiv:2112.08764","publisher":"arXiv","source":"arXiv.org","title":"Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching","type":"article","URL":"http://arxiv.org/abs/2112.08764"},
  {"id":"MehryarMohriFoundations","accessed":{"date-parts":[[2023,1,10]]},"citation-key":"MehryarMohriFoundations","title":"Mehryar Mohri -- Foundations of Machine Learning - Book","type":"webpage","URL":"https://cs.nyu.edu/~mohri/mlbook/"},
  {"id":"mohriFoundationsMachineLearning","author":[{"family":"Mohri","given":"Mehryar"},{"family":"Rostamizadeh","given":"Afshin"},{"family":"Talwalkar","given":"Ameet"}],"citation-key":"mohriFoundationsMachineLearning","language":"en","source":"Zotero","title":"Foundations of Machine Learning (second edition)","type":"article-journal"},
  {"id":"perezFiLMVisualReasoning2017","abstract":"We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","accessed":{"date-parts":[[2023,2,22]]},"author":[{"family":"Perez","given":"Ethan"},{"family":"Strub","given":"Florian"},{"family":"Vries","given":"Harm","non-dropping-particle":"de"},{"family":"Dumoulin","given":"Vincent"},{"family":"Courville","given":"Aaron"}],"citation-key":"perezFiLMVisualReasoning2017","issued":{"date-parts":[[2017,12,18]]},"number":"arXiv:1709.07871","publisher":"arXiv","source":"arXiv.org","title":"FiLM: Visual Reasoning with a General Conditioning Layer","title-short":"FiLM","type":"article","URL":"http://arxiv.org/abs/1709.07871"},
  {"id":"renQuery2boxReasoningKnowledge2020","abstract":"Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on two large KGs and show that query2box achieves up to 25% relative improvement over the state of the art.","author":[{"family":"Ren","given":"Hongyu"},{"family":"Hu","given":"Weihua"},{"family":"Leskovec","given":"Jure"}],"citation-key":"renQuery2boxReasoningKnowledge2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 2002.05969\nMAG ID: 2994695355\nS2ID: 66f7823be7a0adcd8ed3ecebf00e53daf45bc6bc","title":"Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings","type":"article-journal"},
  {"id":"wangLOGICALMESSAGEPASSING2023","abstract":"Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers’ embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph represenation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields the new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning. Our implementation can be found at https://github.com/HKUST-KnowComp/LMPNN.","author":[{"family":"Wang","given":"Zihao"},{"family":"Song","given":"Yangqiu"},{"family":"Wong","given":"Ginny Y"},{"family":"See","given":"Simon"}],"citation-key":"wangLOGICALMESSAGEPASSING2023","issued":{"date-parts":[[2023]]},"language":"en","source":"Zotero","title":"LOGICAL MESSAGE PASSING NETWORKS WITH ONE-HOP INFERENCE ON ATOMIC FORMULAS","type":"article-journal"},
  {"id":"wangLogicalMessagePassing2023","abstract":"Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers' embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph represenation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields the new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Wang","given":"Zihao"},{"family":"Song","given":"Yangqiu"},{"family":"Wong","given":"Ginny Y."},{"family":"See","given":"Simon"}],"citation-key":"wangLogicalMessagePassing2023","DOI":"10.48550/arXiv.2301.08859","issued":{"date-parts":[[2023,2,20]]},"number":"arXiv:2301.08859","publisher":"arXiv","source":"arXiv.org","title":"Logical Message Passing Networks with One-hop Inference on Atomic Formulas","type":"article","URL":"http://arxiv.org/abs/2301.08859"},
  {"id":"yingsuMICOMultialternativeContrastive2022","abstract":"Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a M ulti-alternative contrast I ve learning framework on CO mmonsense knowledge graphs (MICO). MICO generates the commonsense knowledge representation by contextual interaction between entity nodes and relations with multi-alternative contrastive learning. In MICO, the head and tail entities in an ( h, r, t ) knowledge triple are converted to two relation-aware sequence pairs (a premise and an alternative) in the form of natural language. Semantic representations generated by MICO can beneﬁt the following two tasks by simply comparing the distance score between the representations: Extensive experiments show the effectiveness our method.","author":[{"literal":"Ying Su"},{"literal":"Zihao Wang"},{"literal":"Tianqing Fang"},{"literal":"Hongming Zhang"},{"literal":"Yangqiu Song"},{"literal":"T. Zhang"}],"citation-key":"yingsuMICOMultialternativeContrastive2022","container-title":"ArXiv","DOI":"10.48550/arxiv.2210.07570","issued":{"date-parts":[[2022]]},"note":"ARXIV_ID: 2210.07570\nS2ID: 480e667c50eb71ba78bfde47e4686ca7b21148bd","title":"MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation","type":"article-journal"},
  {"id":"yuLearningCountIsomorphisms2023","abstract":"Subgraph isomorphism counting is an important problem on graphs, as many graph-based tasks exploit recurring subgraph patterns. Classical methods usually boil down to a backtracking framework that needs to navigate a huge search space with prohibitive computational costs. Some recent studies resort to graph neural networks (GNNs) to learn a low-dimensional representation for both the query and input graphs, in order to predict the number of subgraph isomorphisms on the input graph. However, typical GNNs employ a node-centric message passing scheme that receives and aggregates messages on nodes, which is inadequate in complex structure matching for isomorphism counting. Moreover, on an input graph, the space of possible query graphs is enormous, and different parts of the input graph will be triggered to match different queries. Thus, expecting a fixed representation of the input graph to match diversely structured query graphs is unrealistic. In this paper, we propose a novel GNN called Count-GNN for subgraph isomorphism counting, to deal with the above challenges. At the edge level, given that an edge is an atomic unit of encoding graph structures, we propose an edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency to preserve fine-grained structural information. At the graph level, we modulate the input graph representation conditioned on the query, so that the input graph can be adapted to each query individually to improve their matching. Finally, we conduct extensive experiments on a number of benchmark datasets to demonstrate the superior performance of Count-GNN.","accessed":{"date-parts":[[2023,2,12]]},"author":[{"family":"Yu","given":"Xingtong"},{"family":"Liu","given":"Zemin"},{"family":"Fang","given":"Yuan"},{"family":"Zhang","given":"Xinming"}],"citation-key":"yuLearningCountIsomorphisms2023","issued":{"date-parts":[[2023,2,7]]},"number":"arXiv:2302.03266","publisher":"arXiv","source":"arXiv.org","title":"Learning to Count Isomorphisms with Graph Neural Networks","type":"article","URL":"http://arxiv.org/abs/2302.03266"},
  {"id":"zhangAddingConditionalControl2023","abstract":"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Zhang","given":"Lvmin"},{"family":"Agrawala","given":"Maneesh"}],"citation-key":"zhangAddingConditionalControl2023","issued":{"date-parts":[[2023,2,10]]},"number":"arXiv:2302.05543","publisher":"arXiv","source":"arXiv.org","title":"Adding Conditional Control to Text-to-Image Diffusion Models","type":"article","URL":"http://arxiv.org/abs/2302.05543"},
  {"id":"zhangRethinkingExpressivePower2023a","abstract":"Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.","accessed":{"date-parts":[[2023,2,19]]},"author":[{"family":"Zhang","given":"Bohang"},{"family":"Luo","given":"Shengjie"},{"family":"Wang","given":"Liwei"},{"family":"He","given":"Di"}],"citation-key":"zhangRethinkingExpressivePower2023a","DOI":"10.48550/arXiv.2301.09505","issued":{"date-parts":[[2023,1,23]]},"number":"arXiv:2301.09505","publisher":"arXiv","source":"arXiv.org","title":"Rethinking the Expressive Power of GNNs via Graph Biconnectivity","type":"article","URL":"http://arxiv.org/abs/2301.09505"}
]
